{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(QuantileEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.fc = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, taus):\n",
    "        N = taus.size(0)\n",
    "        taus = taus.unsqueeze(-1)  # Add an extra dimension\n",
    "        # Create a constant tensor of dimension (embedding_dim, )\n",
    "        i_tensor = torch.arange(self.embedding_dim, device=taus.device).float().unsqueeze(0)\n",
    "        # Broadcast multiply\n",
    "        cos_trans = torch.cos(taus * math.pi * i_tensor)\n",
    "        # Pass through the fully connected layer\n",
    "        quantile_embedding = self.fc(cos_trans)\n",
    "        return quantile_embedding\n",
    "\n",
    "class IQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions, embedding_dim=64):\n",
    "        super(IQN, self).__init__()\n",
    "        self.quantile_embedding = QuantileEmbedding(embedding_dim)\n",
    "        self.layer1 = nn.Linear(n_observations + embedding_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, state, taus):\n",
    "        quantile_embedding = self.quantile_embedding(taus)\n",
    "        # Repeat each element in the state tensor to match the batch size of quantile_embedding\n",
    "        repeated_state = state.repeat_interleave(quantile_embedding.size(0) // state.size(0), dim=0)\n",
    "        combined = torch.cat((repeated_state, quantile_embedding), dim=-1)\n",
    "        x = F.relu(self.layer1(combined))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2364314659.py, line 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 70\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.plot(means.numpy())def select_action(state):\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = IQN(n_observations, n_actions).to(device)\n",
    "target_net = IQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            num_quantiles = 100\n",
    "            taus = torch.linspace(0, 1, num_quantiles, device=device).unsqueeze(0)\n",
    "            action_quantiles = policy_net(state, taus)\n",
    "            mean_action_values = action_quantiles.mean(1)\n",
    "            return mean_action_values.max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            num_quantiles = 100\n",
    "            taus = torch.linspace(0, 1, num_quantiles, device=device).unsqueeze(0)\n",
    "            action_quantiles = policy_net(state, taus)\n",
    "            mean_action_values = action_quantiles.mean(1)\n",
    "            return mean_action_values.max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Convert batch components to tensors\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Sample a set of quantile fractions (τ) for the current batch\n",
    "    taus = torch.rand(BATCH_SIZE, device=device)\n",
    "\n",
    "    # Compute the quantile values for the current state and action pairs\n",
    "    state_action_quantiles = policy_net(state_batch, taus).gather(1, action_batch.unsqueeze(-1).expand(BATCH_SIZE, taus.size(1)))\n",
    "\n",
    "    # Compute the next state values\n",
    "    next_state_quantiles = torch.zeros(BATCH_SIZE, taus.size(1), device=device)\n",
    "    if non_final_mask.sum() > 0:\n",
    "        with torch.no_grad():\n",
    "            next_state_values = target_net(non_final_next_states, taus).max(2)[0]\n",
    "            next_state_quantiles[non_final_mask] = next_state_values\n",
    "\n",
    "    # Compute the target quantile values\n",
    "    expected_quantiles = reward_batch.unsqueeze(1) + (GAMMA * next_state_quantiles)\n",
    "\n",
    "    # Compute the Quantile Regression Loss\n",
    "    td_error = expected_quantiles.unsqueeze(2) - state_action_quantiles.unsqueeze(1)\n",
    "    abs_td_error = td_error.abs()\n",
    "    huber_loss = torch.where(abs_td_error <= 1, 0.5 * td_error.pow(2), abs_td_error - 0.5)\n",
    "    quantile_loss = torch.abs(taus.unsqueeze(1) - (td_error.detach() < 0).float()) * huber_loss\n",
    "    loss = quantile_loss.mean()\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (64) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [100, 64].  Tensor sizes: [1, 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     action \u001b[39m=\u001b[39m select_action(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     observation, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward], device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[1;32m/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m taus \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, num_quantiles, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m# Get the quantile values for all actions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m action_quantiles \u001b[39m=\u001b[39m policy_net(state, taus)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Calculate the mean of the quantiles for each action\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m mean_action_values \u001b[39m=\u001b[39m action_quantiles\u001b[39m.\u001b[39mmean(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bottledetection/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m quantile_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantile_embedding(taus)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Ensure that state and quantile embeddings are correctly shaped\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39;49mexpand_as(quantile_embedding)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m combined \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((state, quantile_embedding), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/bratet/Workspace/IQN-FQF-Distribitional-RL/main.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(combined))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (64) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [100, 64].  Tensor sizes: [1, 4]"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
